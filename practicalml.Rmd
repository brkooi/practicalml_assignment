---
title: "Modeling performance barbell lifts"
author: "Ben Kooi"
date: "16-2-2022"
output:
  html_document: default
  pdf_document: default
---

## Synopsis
The goal of assignment is to analyze how effective the training is of barbell lifting based on the measurements of all kinds of body sensors. The effectiveness is classified as letters 'A' until 'E'.
The classification is modelled with three different algorithms treebagging, random forest and gradient boosting.
The out of sample error of the three algorithms are:  
* treebagging       -> 1.61%
* random forest     -> 0.51%
* gradient boosting -> 3.98%

The outcome of the different models are the same on the 20 cases in the testing-set. The best performing model is the random forest-model and will be used to answer the questions of the Quorse Assignment Prediction Quiz.


## Setting the environment

Het laden van de R-packages en het defineren van variabelen.
```{r message=FALSE}

#laden van packages
packages <- c("caret", "janitor", "dplyr")
sapply(packages, require, character.only=TRUE, quietly=TRUE)

```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Prepare the datasets 

The following steps are taken to prepare the training and validation-datasets:      
1. Read the datasets for training and testing the models;    
2. The testing-dataset is the reference for the training- and validation-dataset, so at first the testing-dataset is cleaned;    
3. A new dataframe 'data' is constructed from the testing-dataset with only 49 of the original 160 variables. Most of the variables had no values at all;      
4. The dataframe 'data' is partitioned to the dataframes 'training' and 'validation'.  


```{r prepareDatasets, cache = TRUE}

# load training and testing datasets where undefined values are converted to NA
data <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", na.strings = c("#DIV/0!", "NA", ""))
testing <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", na.strings = c("#DIV/0!", "NA",""))

# remove empty columns from testing-dataset
testing <- remove_empty(testing, which = "cols")

# read the columnnames from the testing-dataset in a list
columnnames <- names(testing)

# remove the columname 'problem_id' from the list
columnnames <- columnnames[columnnames != "problem_id"]

# add the columname 'classe' to the list
columnnames <- c(columnnames, "classe")

# create a new dataframe from the selected columnnames of the testing-dataset
data <- data %>% select(all_of(columnnames))

# remove unnecessary columns from the dataframe
data <- subset(data, select = -c(X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, num_window, new_window, cvtd_timestamp) )

# remove all the columns within the string 'total' 
data <- data %>% select(-contains(c('total')))

# convert 'classe'-variable to factor
data$classe <- as.factor(data$classe)

# print the structure of the dataframe
str(data)

# create training and validation-sets
trainIndex <- createDataPartition(data$classe, p=0.8, list=FALSE)
training <- data[ trainIndex,]
validation <- data[-trainIndex,]

# construct testing-set
data <- subset(data, select = -c(classe))
columnnames <- names(data)
testing <- testing %>% select(all_of(columnnames))

```

## Model selection

For selecting the most accurate model, three algorithms are chosen:    
* Treebagging;  
* Random Forest;  
* Gradient Boosting.  

For tuning the algorithms, the dependent variable 'classe' will be calculated from all the other independent variables (predictors).

Printing and plotting the models will give information about the accuracy and the best values for the tuning-parameters. These values are used in the final models which are used to validate the outcomes on the validation-dataset. In the final section 'Plots' you can find this information.


```{r modelSelection, cache = TRUE}

set.seed(123)

# train a Treebagging-model with the default parameters and all the variables
modelTb <- train(classe ~ ., data=training, method="treebag")

# train a Random Forest-model with the default parameters and all the variables
modelRf <- train(classe ~ ., data=training, method="rf")

# train a Gradient Boosting-model with the default parameters and all the variables
modelGbm <- train(classe ~ ., data=training, method="gbm", verbose = FALSE)

# print the trained Treebagging-model
modelTb

# print the trained Random Forest-model
modelRf

# print the trained Gradient Boosting-model
modelGbm

```

## Final models

Based on the cross-validation of the different models, the best performing model in this case is Random Forest.

```{r finalModels, cache = TRUE}

set.seed(1234)

# train a Treebagging-model with the default parameters and all the variables
finalmodelTb <- train(classe ~ ., data=training, method="treebag")

# train a Random Forest-model with all the variables
finalmodelRf <- train(classe ~ .
                      , data=training
                      , method="rf"
                      , ntree=150
                      , tuneGrid = expand.grid(.mtry = 25))

# train a Gradient Boosting-model with all the variables
finalmodelGbm <- train(classe ~ .
                       , data=training
                       , method="gbm"
                       , tuneGrid = expand.grid(n.trees=150, 
                                                interaction.depth=3, 
                                                shrinkage=.1, 
                                                n.minobsinnode = 10)
                       ,verbose = FALSE)

# predict and cross-validate the final Treebagging-model
predictTb <- predict(finalmodelTb, validation)
confusionMatrix(predictTb,validation$classe)

# predict and cross-validate the final Random Forest-model
predictRf <- predict(finalmodelRf, validation)
confusionMatrix(predictRf,validation$classe)

# predict and cross-validate the final Random Forest-model
predictGbm <- predict(finalmodelGbm, validation)
confusionMatrix(predictGbm,validation$classe)

```

## Prediction on 20 testcases

The three different algorithms classifies the 20 test-cases with the same values.

``` {r predictTestcases}

predictTestCasesTb <- predict(finalmodelTb, testing)
predictTestCasesTb

predictTestCasesRf <- predict(finalmodelRf, testing)
predictTestCasesRf

predictTestCasesGbm <- predict(finalmodelGbm, testing)
predictTestCasesGbm

```


## Plots

This section shows the plots on which the tuning-parameters are based on.
``` {r plots, cache = TRUE}
plot(modelRf, main="Random Forest - Randomly selected variables vs. Accuracy")
plot(modelRf$finalModel, main="Random Forest - Amount of trees vs. Error-rate")
plot(varImp(modelTb), main="Treebagging - Order of importance of used variables")
plot(varImp(modelRf), main="Random Forest - Order of importance of used variables")
plot(varImp(modelGbm),main="Gradient Boosting - Order of importance of used variables")
```
